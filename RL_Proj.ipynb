{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1335)  # for reproducibility\n",
    "np.set_printoptions(precision=5, suppress=True, linewidth=150)\n",
    "\n",
    "import pandas as pd\n",
    "import backtest as twp\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics, preprocessing\n",
    "from talib.abstract import *\n",
    "from sklearn.externals import joblib\n",
    "import os.path\n",
    "\n",
    "\n",
    "import quandl\n",
    "\n",
    "#Load data\n",
    "def read_convert_data(symbol='GPC'):\n",
    "    fname = 'data/' + symbol + '_1day.pkl'\n",
    "    if (os.path.isfile(fname)):\n",
    "        return\n",
    "    quandl.ApiConfig.api_key = \"GNPQiKazcmAssCAzLaw1\"\n",
    "    prices = quandl.get('WIKI/' + symbol)\n",
    "    prices.to_pickle(fname) # a /data folder must exist\n",
    "    return\n",
    "\n",
    "def load_data(test=False, symbol='GPC', max_data=2000, test_percent=0.15):\n",
    "    prices = pd.read_pickle('data/' + symbol + '_1day.pkl')\n",
    "    prices.rename(columns={'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Volume (BTC)': 'volume'}, inplace=True)\n",
    "    \n",
    "    #prices = np.sin(np.arange(200)/30.0) #sine prices\n",
    "    \n",
    "    total = max_data if max_data < prices.shape[0] else prices.shape[0]\n",
    "    end = int(total * test_percent)\n",
    "\n",
    "    \n",
    "    x_train = prices.iloc[-total:-end,]\n",
    "    x_test= prices.iloc[-total:,]\n",
    "    if test:\n",
    "        return x_test\n",
    "    else:\n",
    "        return x_train\n",
    "\n",
    "#Initialize first state, all items are placed deterministically\n",
    "def init_state(indata, test=False):\n",
    "    close = indata['close'].values\n",
    "    diff = np.diff(close)\n",
    "    diff = np.insert(diff, 0, 0)\n",
    "    sma15 = SMA(indata, timeperiod=15)\n",
    "    sma60 = SMA(indata, timeperiod=60)\n",
    "    rsi = RSI(indata, timeperiod=14)\n",
    "    atr = ATR(indata, timeperiod=14)\n",
    "\n",
    "    #--- Preprocess data\n",
    "    xdata = np.column_stack((close, diff, sma15, close-sma15, sma15-sma60, rsi, atr))\n",
    "    \n",
    "    xdata = np.nan_to_num(xdata)\n",
    "    if test == False:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        xdata = np.expand_dims(scaler.fit_transform(xdata), axis=1)\n",
    "        joblib.dump(scaler, 'data/scaler.pkl')\n",
    "    elif test == True:\n",
    "        scaler = joblib.load('data/scaler.pkl')\n",
    "        xdata = np.expand_dims(scaler.fit_transform(xdata), axis=1)\n",
    "    state = xdata[0:1, 0:1, :]\n",
    "    \n",
    "    return state, xdata, close\n",
    "\n",
    "#Take Action\n",
    "def take_action(state, xdata, action, signal, time_step):\n",
    "    #this should generate a list of trade signals that at evaluation time are fed to the backtester\n",
    "    #the backtester should get a list of trade signals and a list of price data for the assett\n",
    "    \n",
    "    #make necessary adjustments to state and then return it\n",
    "    time_step += 1\n",
    "    \n",
    "    #if the current iteration is the last state (\"terminal state\") then set terminal_state to 1\n",
    "    #TODO sell all stocks here?\n",
    "    if time_step + 1 == xdata.shape[0]:\n",
    "        state = xdata[time_step-1:time_step, 0:1, :]\n",
    "        terminal_state = 1\n",
    "        #stocks_owned = np.sum(signal)\n",
    "        #signal.loc[time_step] = -stocks_owned\n",
    "        signal.loc[time_step] = 0\n",
    "\n",
    "        return state, time_step, signal, terminal_state\n",
    "\n",
    "    #move the market data window one step forward\n",
    "    state = xdata[time_step-1:time_step, 0:1, :]\n",
    "    #take action\n",
    "    if action == 1:\n",
    "        signal.loc[time_step] = 100\n",
    "    elif action == 2:\n",
    "        signal.loc[time_step] = -100\n",
    "    else:\n",
    "        signal.loc[time_step] = 0\n",
    "    #print(state)\n",
    "    terminal_state = 0\n",
    "    #print(signal)\n",
    "\n",
    "    return state, time_step, signal, terminal_state\n",
    "\n",
    "#Get Reward, the reward is returned at the end of an episode\n",
    "def get_reward(new_state, time_step, action, xdata, signal, terminal_state, eval=False, epoch=0):\n",
    "    reward = 0\n",
    "    signal.fillna(value=0, inplace=True)\n",
    "    bt = twp.Backtest(pd.Series(data=[x for x in xdata[time_step-2:time_step]], index=signal[time_step-2:time_step].index.values), signal[time_step-2:time_step], signalType='shares')\n",
    "\n",
    "    if eval == False:\n",
    "        reward = ((bt.data['price'].iloc[-1] - bt.data['price'].iloc[-2])*bt.data['shares'].iloc[-1])\n",
    "\n",
    "    if terminal_state == 1 and eval == True:\n",
    "        bt = twp.Backtest(pd.Series(data=[x for x in xdata], index=signal.index.values), signal, signalType='shares')\n",
    "        reward = bt.pnl.iloc[-1]\n",
    "\n",
    "    return reward, bt\n",
    "\n",
    "def plot_epoch(bt, epoch, plt_path, test_percent=0.15):\n",
    "    plt.figure(figsize=(10,5)) #TODO make larger\n",
    "    bt.plotTrades()\n",
    "    split = len(bt.pnl) - int(len(bt.pnl)*test_percent)\n",
    "    plt.axvline(x=split, color='black', linestyle='--')\n",
    "    #plt.text(split-300, 20, 'training data') \n",
    "    #plt.text(split+50, 20, 'test data')\n",
    "    plt.suptitle(str(epoch))\n",
    "    plt.savefig(plt_path + '/'+str(epoch)+'.png', bbox_inches='tight', pad_inches=1, dpi=72)\n",
    "    plt.close('all')\n",
    "\n",
    "def evaluate_Q(eval_data, eval_model, price_data, epoch=0):\n",
    "    #This function is used to evaluate the performance of the system each epoch, without the influence of epsilon and random actions\n",
    "    signal = pd.Series(index=np.arange(len(eval_data)))\n",
    "    state, xdata, price_data = init_state(eval_data)\n",
    "    status = 1\n",
    "    terminal_state = 0\n",
    "    time_step = 1\n",
    "    avg_qval = 0\n",
    "    while(status == 1):\n",
    "        #We start in state S\n",
    "        #Run the Q function on S to get predicted reward values on all the possible actions\n",
    "        qval = eval_model.predict(state, batch_size=1)\n",
    "        avg_qval += np.mean(qval)\n",
    "        action = (np.argmax(qval))\n",
    "        #Take action, observe new state S'\n",
    "        new_state, time_step, signal, terminal_state = take_action(state, xdata, action, signal, time_step)\n",
    "        #Observe reward\n",
    "        eval_reward, bt = get_reward(new_state, time_step, action, price_data, signal, terminal_state, eval=True, epoch=epoch)\n",
    "        state = new_state\n",
    "        if terminal_state == 1: #terminal state\n",
    "            status = 0\n",
    "\n",
    "    return eval_reward, bt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1335)  # for reproducibility\n",
    "np.set_printoptions(precision=5, suppress=True, linewidth=150)\n",
    "\n",
    "import pandas as pd\n",
    "import backtest as twp\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import metrics, preprocessing\n",
    "\n",
    "'''\n",
    "Name:        The Self Learning Quant, Example 2\n",
    "\n",
    "Author:      Daniel Zakrisson\n",
    "\n",
    "Created:     30/03/2016\n",
    "Copyright:   (c) Daniel Zakrisson 2016\n",
    "Licence:     BSD\n",
    "\n",
    "Requirements:\n",
    "Numpy\n",
    "Pandas\n",
    "MatplotLib\n",
    "scikit-learn\n",
    "Keras, https://keras.io/\n",
    "backtest.py from the TWP library. Download backtest.py and put in the same folder\n",
    "\n",
    "/plt create a subfolder in the same directory where plot files will be saved\n",
    "\n",
    "'''\n",
    "\n",
    "def load_data(test=True, symbol='NIHD', max_data=900, test_percent=0.15):\n",
    "    #prices = pd.read_pickle('data/' + symbol + '_1day.pkl')\n",
    "    #prices.rename(columns={'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Volume (BTC)': 'volume'}, inplace=True)\n",
    "    \n",
    "    prices = np.sin(np.arange(200)/30.0) #sine prices\n",
    "    return prices\n",
    "    \n",
    "    #total = max_data if max_data < prices.shape[0] else prices.shape[0]\n",
    "    #end = int(total * test_percent)\n",
    "\n",
    "    \n",
    "    #x_train = prices.iloc[-total:-end,]\n",
    "    #x_test= prices.iloc[-total:,]\n",
    "    #if test:\n",
    "    #    return x_test.close\n",
    "    #else:\n",
    "    #    return x_train.close\n",
    "\n",
    "\n",
    "#Load data\n",
    "#def load_data():\n",
    "#    price = np.sin(np.arange(200)/30.0) #sine prices\n",
    "#    return price\n",
    "\n",
    "#Initialize first state, all items are placed deterministically\n",
    "def init_state(data):\n",
    "    \n",
    "    close = data\n",
    "    diff = np.diff(data)\n",
    "    diff = np.insert(diff, 0, 0)\n",
    "    \n",
    "    #--- Preprocess data\n",
    "    xdata = np.column_stack((close, diff))\n",
    "    xdata = np.nan_to_num(xdata)\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    xdata = scaler.fit_transform(xdata)\n",
    "    \n",
    "    state = xdata[0:1, :]\n",
    "    return state, xdata\n",
    "\n",
    "#Take Action\n",
    "def take_action(state, xdata, action, signal, time_step):\n",
    "    #this should generate a list of trade signals that at evaluation time are fed to the backtester\n",
    "    #the backtester should get a list of trade signals and a list of price data for the assett\n",
    "    \n",
    "    #make necessary adjustments to state and then return it\n",
    "    time_step += 1\n",
    "    \n",
    "    #if the current iteration is the last state (\"terminal state\") then set terminal_state to 1\n",
    "    if time_step == xdata.shape[0]:\n",
    "        state = xdata[time_step-1:time_step, :]\n",
    "        terminal_state = 1\n",
    "        signal.loc[time_step] = 0\n",
    "        return state, time_step, signal, terminal_state\n",
    "\n",
    "    #move the market data window one step forward\n",
    "    state = xdata[time_step-1:time_step, :]\n",
    "    #take action\n",
    "    if action == 0:\n",
    "        signal.loc[time_step] = -100\n",
    "    elif action == 1:\n",
    "        signal.loc[time_step] = 0\n",
    "    elif action == 2:\n",
    "        signal.loc[time_step] = 100\n",
    "    terminal_state = 0\n",
    "\n",
    "    return state, time_step, signal, terminal_state\n",
    "\n",
    "#Get Reward, the reward is returned at the end of an episode\n",
    "def get_reward(new_state, time_step, action, xdata, signal, terminal_state, epoch=0):\n",
    "    reward = 0\n",
    "    signal.fillna(value=0, inplace=True)\n",
    "    if terminal_state == 0:\n",
    "        #get reward for the most current action\n",
    "        if signal[time_step] != signal[time_step-1] and terminal_state == 0:\n",
    "            i=1\n",
    "            while signal[time_step-i] == signal[time_step-1-i] and time_step - 1 - i > 0:\n",
    "                i += 1\n",
    "            reward = (xdata[time_step-1, 0] - xdata[time_step - i-1, 0]) * signal[time_step - 1]*-100 + i*np.abs(signal[time_step - 1])/10.0\n",
    "        if signal[time_step] == 0 and signal[time_step - 1] == 0:\n",
    "            reward -= 10\n",
    "\n",
    "    #calculate the reward for all actions if the last iteration in set\n",
    "    if terminal_state == 1:\n",
    "        #run backtest, send list of trade signals and asset data to backtest function\n",
    "        bt = twp.Backtest(pd.Series(data=[x[0] for x in xdata]), signal, signalType='shares')\n",
    "        reward = bt.pnl.iloc[-1]\n",
    "\n",
    "    return reward\n",
    "\n",
    "def evaluate_Q(eval_data, eval_model):\n",
    "    #This function is used to evaluate the perofrmance of the system each epoch, without the influence of epsilon and random actions\n",
    "    signal = pd.Series(index=np.arange(len(eval_data)))\n",
    "    state, xdata = init_state(eval_data)\n",
    "    status = 1\n",
    "    terminal_state = 0\n",
    "    time_step = 1\n",
    "    while(status == 1):\n",
    "        #We start in state S\n",
    "        #Run the Q function on S to get predicted reward values on all the possible actions\n",
    "        qval = eval_model.predict(state.reshape(1,2,1), batch_size=1)\n",
    "        action = (np.argmax(qval))\n",
    "        #Take action, observe new state S'\n",
    "        new_state, time_step, signal, terminal_state = take_action(state, xdata, action, signal, time_step)\n",
    "        #Observe reward\n",
    "        eval_reward = get_reward(new_state, time_step, action, xdata, signal, terminal_state, i)\n",
    "        state = new_state\n",
    "        if terminal_state == 1: #terminal state\n",
    "            status = 0\n",
    "    return eval_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, return_sequences=True, stateful=False, input_shape=(2,1)))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.2)) I'm not using dropout in this example\n",
    "\n",
    "model.add(LSTM(4, return_sequences=False, stateful=False, input_shape=(2,1)))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(3, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('linear')) #linear output so we can have range of real-valued outputs\n",
    "\n",
    "rms = RMSprop(decay=0.00001) #TODO decay???\n",
    "model.compile(loss='mse', optimizer=rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "target_model = Sequential()\n",
    "target_model.add(LSTM(4, return_sequences=True, stateful=False, input_shape=(2,1)))\n",
    "target_model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.2)) I'm not using dropout in this example\n",
    "\n",
    "target_model.add(LSTM(4, return_sequences=False, stateful=False, input_shape=(2,1)))\n",
    "target_model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "target_model.add(Dense(3, kernel_initializer='lecun_uniform'))\n",
    "target_model.add(Activation('linear')) #linear output so we can have range of real-valued outputs\n",
    "\n",
    "target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #: 0 Reward: 79.304492 Epsilon: 1.000000\n",
      "Epoch #: 1 Reward: 0.000000 Epsilon: 0.995000\n",
      "Epoch #: 2 Reward: 0.000000 Epsilon: 0.990000\n",
      "Epoch #: 3 Reward: 0.000000 Epsilon: 0.985000\n",
      "Epoch #: 4 Reward: 0.000000 Epsilon: 0.980000\n",
      "Epoch #: 5 Reward: 0.000000 Epsilon: 0.975000\n",
      "Epoch #: 6 Reward: 0.000000 Epsilon: 0.970000\n",
      "Epoch #: 7 Reward: 0.000000 Epsilon: 0.965000\n",
      "Epoch #: 8 Reward: 142.631785 Epsilon: 0.960000\n",
      "Epoch #: 9 Reward: 0.000000 Epsilon: 0.955000\n",
      "Epoch #: 10 Reward: 0.000000 Epsilon: 0.950000\n",
      "Epoch #: 11 Reward: 0.000000 Epsilon: 0.945000\n",
      "Epoch #: 12 Reward: 63.241613 Epsilon: 0.940000\n",
      "Epoch #: 13 Reward: 0.000000 Epsilon: 0.935000\n",
      "Epoch #: 14 Reward: 0.000000 Epsilon: 0.930000\n",
      "Epoch #: 15 Reward: 170.414279 Epsilon: 0.925000\n",
      "Epoch #: 16 Reward: 0.000000 Epsilon: 0.920000\n",
      "Epoch #: 17 Reward: 124.807522 Epsilon: 0.915000\n",
      "Epoch #: 18 Reward: 194.377352 Epsilon: 0.910000\n",
      "Epoch #: 19 Reward: 186.316596 Epsilon: 0.905000\n",
      "Epoch #: 20 Reward: 210.639159 Epsilon: 0.900000\n",
      "Epoch #: 21 Reward: 191.917174 Epsilon: 0.895000\n",
      "Epoch #: 22 Reward: 164.319485 Epsilon: 0.890000\n",
      "Epoch #: 23 Reward: 185.523811 Epsilon: 0.885000\n",
      "Epoch #: 24 Reward: 191.917174 Epsilon: 0.880000\n",
      "Epoch #: 25 Reward: 206.330005 Epsilon: 0.875000\n",
      "Epoch #: 26 Reward: 201.314984 Epsilon: 0.870000\n",
      "Epoch #: 27 Reward: 196.556454 Epsilon: 0.865000\n",
      "Epoch #: 28 Reward: 208.179266 Epsilon: 0.860000\n",
      "Epoch #: 29 Reward: 278.335015 Epsilon: 0.855000\n",
      "Epoch #: 30 Reward: 342.149434 Epsilon: 0.850000\n",
      "Epoch #: 31 Reward: 362.124439 Epsilon: 0.845000\n"
     ]
    }
   ],
   "source": [
    "import random, timeit\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "indata = load_data()\n",
    "epochs = 200\n",
    "gamma = 0.9 #a high gamma makes a long term reward more valuable\n",
    "epsilon = 1\n",
    "target_net_update_frequency = 30 #update every 30 days\n",
    "learning_progress = []\n",
    "#stores tuples of (S, A, R, S')\n",
    "h = 0\n",
    "signal = pd.Series(index=np.arange(len(indata)))\n",
    "for i in range(epochs):\n",
    "\n",
    "    state, xdata = init_state(indata)\n",
    "    status = 1\n",
    "    terminal_state = 0\n",
    "    time_step = 1\n",
    "    #while learning is still in progress\n",
    "    while(status == 1):\n",
    "        #We start in state S\n",
    "        #Run the Q function on S to get predicted reward values on all the possible actions\n",
    "        qval = model.predict(state.reshape(1,2,1), batch_size=1)\n",
    "        if (random.random() < epsilon) and i != epochs - 1: #maybe choose random action if not the last epoch\n",
    "            action = np.random.randint(0,3) #assumes 4 different actions\n",
    "        else: #choose best action from Q(s,a) values\n",
    "            action = (np.argmax(qval))\n",
    "        #Take action, observe new state S'\n",
    "        new_state, time_step, signal, terminal_state = take_action(state, xdata, action, signal, time_step)\n",
    "        #Observe reward\n",
    "        reward = get_reward(new_state, time_step, action, xdata, signal, terminal_state, i)\n",
    "        #Get max_Q(S',a)\n",
    "        newQ = target_model.predict(new_state.reshape(1,2,1), batch_size=1)\n",
    "        maxQ = np.max(newQ)\n",
    "        y = np.zeros((1,3))\n",
    "        y[:] = qval[:]\n",
    "        if terminal_state == 0: #non-terminal state\n",
    "            update = (reward + (gamma * maxQ))\n",
    "        else: #terminal state (means that it is the last state)\n",
    "            update = reward\n",
    "        y[0][action] = update #target output\n",
    "        model.fit(state.reshape(1,2,1), y, batch_size=1, nb_epoch=1, verbose=0)\n",
    "        state = new_state\n",
    "        if terminal_state == 1: #terminal state\n",
    "            status = 0\n",
    "            \n",
    "        if time_step % target_net_update_frequency == 0:\n",
    "            weights = model.get_weights()\n",
    "            target_model.set_weights(weights)\n",
    "            \n",
    "    eval_reward = evaluate_Q(indata, model)\n",
    "    print(\"Epoch #: %s Reward: %f Epsilon: %f\" % (i,eval_reward, epsilon))\n",
    "    learning_progress.append((eval_reward))\n",
    "    if epsilon > 0.1:\n",
    "        epsilon -= (1.0/epochs)\n",
    "\n",
    "elapsed = np.round(timeit.default_timer() - start_time, decimals=2)\n",
    "print(\"Completed in %f\" % (elapsed,))\n",
    "\n",
    "#plot results\n",
    "bt = twp.Backtest(pd.Series(data=[x[0] for x in xdata]), signal, signalType='shares')\n",
    "bt.data['delta'] = bt.data['shares'].diff().fillna(0)\n",
    "\n",
    "print(bt.data)\n",
    "\n",
    "plt.figure()\n",
    "bt.plotTrades()\n",
    "plt.suptitle('epoch' + str(i))\n",
    "plt.savefig('plt/final_trades'+'.png', bbox_inches='tight', pad_inches=1, dpi=72) #assumes there is a ./plt dir\n",
    "plt.close('all')\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "bt.plotTrades()\n",
    "plt.subplot(3,1,2)\n",
    "bt.pnl.plot(style='x-')\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(learning_progress)\n",
    "\n",
    "plt.savefig('plt/summary'+'.png', bbox_inches='tight', pad_inches=1, dpi=72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This neural network is the the Q-function, run it like this:\n",
    "#model.predict(state.reshape(1,64), batch_size=1)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import keras.backend as K\n",
    "\n",
    "tsteps = 1\n",
    "batch_size = 1\n",
    "num_features = 7\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='relu',\n",
    "               input_shape=(1, num_features),\n",
    "               return_sequences=True,\n",
    "               stateful=False))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model.add(LSTM(64, activation='relu',\n",
    "               input_shape=(1, num_features),\n",
    "               return_sequences=False,\n",
    "               stateful=False))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(4, activation='linear', kernel_initializer='lecun_uniform'))\n",
    "\n",
    "#model.add(Lambda(lambda a: K.expand_dims(a[:, 0], axis=-1) + a[:, 1:] - K.mean(a[:, 1:], \n",
    "#                keepdims=True), output_shape=(3,)))\n",
    "\n",
    "\n",
    "rms = RMSprop()\n",
    "adam = Adam(decay=0.5) #TODO learning rate decay?\n",
    "model.compile(loss='mse', optimizer=adam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = Sequential()\n",
    "target_model.add(LSTM(64,\n",
    "               input_shape=(1, num_features),\n",
    "               return_sequences=True,\n",
    "               stateful=False))\n",
    "target_model.add(Dropout(0.5))\n",
    "\n",
    "target_model.add(LSTM(64,\n",
    "               input_shape=(1, num_features),\n",
    "               return_sequences=False,\n",
    "               stateful=False))\n",
    "target_model.add(Dropout(0.5))\n",
    "\n",
    "target_model.add(Dense(4, kernel_initializer='lecun_uniform'))\n",
    "\n",
    "#target_model.add(Lambda(lambda a: K.expand_dims(a[:, 0], axis=-1) + a[:, 1:] - K.mean(a[:, 1:], \n",
    "#                keepdims=True), output_shape=(3,)))\n",
    "\n",
    "target_model.add(Activation('linear')) #linear output so we can have range of real-valued outputs\n",
    "\n",
    "#rms = RMSprop()\n",
    "#adam = Adam()\n",
    "#target_model.compile(loss='mse', optimizer=adam)\n",
    "target_model.set_weights(model.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: write first half of paper for RL\n",
    "#TODO: add double network (below) and dueling network (above)\n",
    "#TODO: copy most of above code and some of below code to implement DDPG and A2C\n",
    "#TODO: code to compare to baseline and plot\n",
    "#TODO: experiment with different indicators, architectures, actions, rewards, number of stocks, exploration, etc.\n",
    "#TODO: implement better batch drawing mechanic (such as from DDPG)\n",
    "#TODO: clean up code\n",
    "#TODO: compare all models (include different versions of QNN, dueling, double, recurrent, etc)\n",
    "#TODO: write both papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = 'NIHD'\n",
    "#TODO run with other increasing stocks (AAPL, GOOG, GNC, etc.)\n",
    "#TODO run with NIHD stock (decreasing over time, AMD?, etc) and only 1000 days\n",
    "#TODO run with cyclycal stock (UNP?, WLK?, CENX?, etc.)\n",
    "read_convert_data(symbol=symbol) #run once to read indata, resample and convert to pickle (if doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_percent = 0.15\n",
    "#max_data = 1825 #5 years (minus leap years)\n",
    "max_data = 900 # 3 years\n",
    "\n",
    "train_data = load_data(test=False, symbol=symbol, max_data=max_data, test_percent=test_percent)\n",
    "test_data = load_data(test=True, symbol=symbol, max_data=max_data, test_percent=test_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAEyCAYAAADeAVWKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZhm4mTIzMzPLwM2UmZmZWQb/A+l+UROaL7fvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd0c36ef890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = test_data.values\n",
    "plt.figure(figsize=(10,5)) \n",
    "plt.plot(d)\n",
    "split = len(test_data) - int(len(test_data)*test_percent)\n",
    "plt.axvline(x=split, color='black', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Reward: -21607.500000 eps: 1.000000 Time: 115.204980 (s): Avg Train Q-Val: 1.639802\n",
      "Epoch: 1 Reward: -23712.900000 eps: 0.990000 Time: 165.718333 (s): Avg Train Q-Val: 14.876519\n",
      "Epoch: 2 Reward: 1894.000000 eps: 0.980000 Time: 177.340021 (s): Avg Train Q-Val: 25.087395\n",
      "Epoch: 3 Reward: -299.700000 eps: 0.970000 Time: 186.483468 (s): Avg Train Q-Val: 27.580300\n",
      "Epoch: 4 Reward: 1459.400000 eps: 0.960000 Time: 164.808136 (s): Avg Train Q-Val: 27.436875\n",
      "Epoch: 5 Reward: -9439.500000 eps: 0.950000 Time: 167.200350 (s): Avg Train Q-Val: 37.584313\n",
      "Epoch: 6 Reward: -3141.500000 eps: 0.940000 Time: 158.165347 (s): Avg Train Q-Val: 45.370919\n",
      "Epoch: 7 Reward: 495.000000 eps: 0.930000 Time: 167.435090 (s): Avg Train Q-Val: 39.735374\n",
      "Epoch: 8 Reward: 10241.500000 eps: 0.920000 Time: 170.484112 (s): Avg Train Q-Val: 41.437825\n",
      "Epoch: 9 Reward: -4456.500000 eps: 0.910000 Time: 158.884196 (s): Avg Train Q-Val: 52.472400\n",
      "Epoch: 10 Reward: -12338.000000 eps: 0.900000 Time: 159.016404 (s): Avg Train Q-Val: 44.662269\n",
      "Epoch: 11 Reward: -12043.000000 eps: 0.890000 Time: 158.051605 (s): Avg Train Q-Val: 44.571080\n",
      "Epoch: 12 Reward: 4840.100000 eps: 0.880000 Time: 158.398887 (s): Avg Train Q-Val: 43.971164\n",
      "Epoch: 13 Reward: -4265.000000 eps: 0.870000 Time: 159.068767 (s): Avg Train Q-Val: 50.056642\n",
      "Epoch: 14 Reward: -1283.500000 eps: 0.860000 Time: 158.249206 (s): Avg Train Q-Val: 62.580435\n",
      "Epoch: 15 Reward: -569.600000 eps: 0.850000 Time: 158.596395 (s): Avg Train Q-Val: 58.301105\n",
      "Epoch: 16 Reward: 8418.600000 eps: 0.840000 Time: 159.113384 (s): Avg Train Q-Val: 61.418641\n",
      "Epoch: 17 Reward: 3611.000000 eps: 0.830000 Time: 158.270672 (s): Avg Train Q-Val: 63.695222\n",
      "Epoch: 18 Reward: 15009.100000 eps: 0.820000 Time: 170.965491 (s): Avg Train Q-Val: 63.237322\n",
      "Epoch: 19 Reward: 2.600000 eps: 0.810000 Time: 159.293059 (s): Avg Train Q-Val: 68.303101\n",
      "Epoch: 20 Reward: 919.000000 eps: 0.800000 Time: 164.841244 (s): Avg Train Q-Val: 78.313282\n",
      "Epoch: 21 Reward: -31220.000000 eps: 0.790000 Time: 160.520819 (s): Avg Train Q-Val: 75.543812\n",
      "Epoch: 22 Reward: -5055.000000 eps: 0.780000 Time: 161.256497 (s): Avg Train Q-Val: 78.317823\n",
      "Epoch: 23 Reward: -11535.500000 eps: 0.770000 Time: 165.110122 (s): Avg Train Q-Val: 68.218360\n",
      "Epoch: 24 Reward: 6712.600000 eps: 0.760000 Time: 184.163404 (s): Avg Train Q-Val: 81.435188\n",
      "Epoch: 25 Reward: 46697.600000 eps: 0.750000 Time: 161.555242 (s): Avg Train Q-Val: 77.440619\n",
      "Epoch: 26 Reward: 9021.000000 eps: 0.740000 Time: 161.040967 (s): Avg Train Q-Val: 75.005519\n",
      "Epoch: 27 Reward: 7818.000000 eps: 0.730000 Time: 161.848107 (s): Avg Train Q-Val: 84.301181\n",
      "Epoch: 28 Reward: 3267.000000 eps: 0.720000 Time: 161.608500 (s): Avg Train Q-Val: 98.008116\n",
      "Epoch: 29 Reward: -4040.600000 eps: 0.710000 Time: 160.761108 (s): Avg Train Q-Val: 89.341183\n",
      "Epoch: 30 Reward: 24235.600000 eps: 0.700000 Time: 160.980059 (s): Avg Train Q-Val: 82.157283\n",
      "Epoch: 31 Reward: 14744.000000 eps: 0.690000 Time: 160.959780 (s): Avg Train Q-Val: 84.086016\n",
      "Epoch: 32 Reward: -2168.000000 eps: 0.680000 Time: 175.500941 (s): Avg Train Q-Val: 91.528750\n",
      "Epoch: 33 Reward: 7238.000000 eps: 0.670000 Time: 183.423990 (s): Avg Train Q-Val: 100.049516\n",
      "Epoch: 34 Reward: 36102.900000 eps: 0.660000 Time: 168.854133 (s): Avg Train Q-Val: 90.857419\n",
      "Epoch: 35 Reward: -7998.000000 eps: 0.650000 Time: 158.627763 (s): Avg Train Q-Val: 102.496767\n",
      "Epoch: 36 Reward: 27036.000000 eps: 0.640000 Time: 158.819910 (s): Avg Train Q-Val: 102.902785\n",
      "Epoch: 37 Reward: 26270.500000 eps: 0.630000 Time: 157.607353 (s): Avg Train Q-Val: 94.910646\n",
      "Epoch: 38 Reward: 53789.500000 eps: 0.620000 Time: 157.302015 (s): Avg Train Q-Val: 96.184710\n",
      "Epoch: 39 Reward: 40260.000000 eps: 0.610000 Time: 157.108245 (s): Avg Train Q-Val: 86.131737\n",
      "Epoch: 40 Reward: 51906.600000 eps: 0.600000 Time: 157.150131 (s): Avg Train Q-Val: 87.282271\n",
      "Epoch: 41 Reward: 49471.900000 eps: 0.590000 Time: 188.535924 (s): Avg Train Q-Val: 77.974781\n",
      "Epoch: 42 Reward: 32160.200000 eps: 0.580000 Time: 180.545708 (s): Avg Train Q-Val: 67.841702\n",
      "Epoch: 43 Reward: 7047.200000 eps: 0.570000 Time: 201.232110 (s): Avg Train Q-Val: 75.855836\n",
      "Epoch: 44 Reward: 471.600000 eps: 0.560000 Time: 167.002838 (s): Avg Train Q-Val: 75.091751\n",
      "Epoch: 45 Reward: 24218.100000 eps: 0.550000 Time: 164.492575 (s): Avg Train Q-Val: 75.662777\n",
      "Epoch: 46 Reward: 50067.600000 eps: 0.540000 Time: 163.954018 (s): Avg Train Q-Val: 65.243186\n",
      "Epoch: 47 Reward: 10308.000000 eps: 0.530000 Time: 169.732764 (s): Avg Train Q-Val: 64.662581\n",
      "Epoch: 48 Reward: -4785.400000 eps: 0.520000 Time: 179.753903 (s): Avg Train Q-Val: 58.894801\n",
      "Epoch: 49 Reward: 22533.500000 eps: 0.510000 Time: 235.183369 (s): Avg Train Q-Val: 59.719955\n",
      "Epoch: 50 Reward: 13435.500000 eps: 0.500000 Time: 278.432586 (s): Avg Train Q-Val: 65.543099\n",
      "Epoch: 51 Reward: 20692.000000 eps: 0.490000 Time: 278.484710 (s): Avg Train Q-Val: 66.520469\n",
      "Epoch: 52 Reward: -7638.600000 eps: 0.480000 Time: 286.166383 (s): Avg Train Q-Val: 71.410752\n",
      "Epoch: 53 Reward: -32193.000000 eps: 0.470000 Time: 287.299293 (s): Avg Train Q-Val: 70.075324\n",
      "Epoch: 54 Reward: -19955.100000 eps: 0.460000 Time: 290.553106 (s): Avg Train Q-Val: 82.722108\n",
      "Epoch: 55 Reward: -23452.500000 eps: 0.450000 Time: 287.929438 (s): Avg Train Q-Val: 81.425069\n",
      "Epoch: 56 Reward: -18920.000000 eps: 0.440000 Time: 280.809730 (s): Avg Train Q-Val: 78.020335\n",
      "Epoch: 57 Reward: -30046.200000 eps: 0.430000 Time: 283.386784 (s): Avg Train Q-Val: 83.076261\n",
      "Epoch: 58 Reward: -17623.500000 eps: 0.420000 Time: 289.657554 (s): Avg Train Q-Val: 76.948977\n",
      "Epoch: 59 Reward: -19707.400000 eps: 0.410000 Time: 288.648239 (s): Avg Train Q-Val: 67.333278\n",
      "Epoch: 60 Reward: -7281.900000 eps: 0.400000 Time: 291.662589 (s): Avg Train Q-Val: 61.574125\n",
      "Epoch: 61 Reward: -35635.600000 eps: 0.390000 Time: 284.525568 (s): Avg Train Q-Val: 69.548738\n",
      "Epoch: 62 Reward: -5916.500000 eps: 0.380000 Time: 293.067488 (s): Avg Train Q-Val: 66.007647\n",
      "Epoch: 63 Reward: -25391.600000 eps: 0.370000 Time: 289.173998 (s): Avg Train Q-Val: 61.200427\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-350-32e150505e75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;31m#Get max_Q(S',a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mold_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mold_qval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0;31m#TODO: use target_model to predict these q values?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;31m#newQ = target_model.predict(new_state, batch_size=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/.conda/envs/proj/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    911\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/.conda/envs/proj/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1711\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1713\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/home/mike/.conda/envs/proj/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/.conda/envs/proj/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/.conda/envs/proj/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/.conda/envs/proj/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/.conda/envs/proj/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/.conda/envs/proj/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/.conda/envs/proj/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random, timeit\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "plt_path = 'plt_qnn_increasing'\n",
    "\n",
    "if not os.path.exists(plt_path):\n",
    "    os.makedirs(plt_path)\n",
    "\n",
    "epochs = 100\n",
    "gamma = 0.95 #since the reward can be several time steps away, make gamma high\n",
    "epsilon = 1\n",
    "batchSize = 100\n",
    "buffer = 200\n",
    "replay = []\n",
    "learning_progress = []\n",
    "target_net_update_frequency = 30 # every 30 days\n",
    "tau = 0.001\n",
    "#stores tuples of (S, A, R, S')\n",
    "h = 0\n",
    "#signal = pd.Series(index=market_data.index)\n",
    "signal = pd.Series(index=np.arange(len(train_data)))\n",
    "avg_qval_list = []\n",
    "test_avg_qval_list = []\n",
    "\n",
    "buff = ReplayBuffer(BUFFER_SIZE)    #Create replay buffer\n",
    "#TODO add ReplayBuffer support below to train on 1 batch at time instead of 1 sample at time\n",
    "\n",
    "for i in range(epochs):\n",
    "    epoch_start_time = timeit.default_timer()\n",
    "\n",
    "    if i == epochs-1: #the last epoch, use test data set\n",
    "        #indata = load_data(test=True)\n",
    "        state, xdata, price_data = init_state(test_data, test=True)\n",
    "    else:\n",
    "        state, xdata, price_data = init_state(train_data)\n",
    "    status = 1\n",
    "    terminal_state = 0\n",
    "    #time_step = market_data.index[0] + 64 #when using market_data\n",
    "    time_step = 14\n",
    "    #while game still in progress\n",
    "    avg_qval = 0\n",
    "    steps = 0\n",
    "    while(status == 1):\n",
    "        #We are in state S\n",
    "        #Let's run our Q function on S to get Q values for all possible actions\n",
    "        qval = model.predict(state, batch_size=1)\n",
    "        #avg_qval += np.mean(qval) #TODO mean of qval or max of qval?\n",
    "        avg_qval += np.max(qval) \n",
    "\n",
    "        if (random.random() < epsilon): #choose random action\n",
    "            action = np.random.randint(0,4) #assumes 4 different actions\n",
    "        else: #choose best action from Q(s,a) values\n",
    "            action = (np.argmax(qval))\n",
    "        #Take action, observe new state S'\n",
    "        new_state, time_step, signal, terminal_state = take_action(state, xdata, action, signal, time_step)\n",
    "        #Observe reward\n",
    "        reward, bt = get_reward(new_state, time_step, action, price_data, signal, terminal_state)\n",
    "\n",
    "        #Experience replay storage\n",
    "        if (len(replay) < buffer): #if buffer not filled, add to it\n",
    "            replay.append((state, action, reward, new_state))\n",
    "            #print(time_step, reward, terminal_state)\n",
    "        else: #if buffer full, overwrite old values\n",
    "            if (h < (buffer-1)):\n",
    "                h += 1\n",
    "            else:\n",
    "                h = 0\n",
    "            replay[h] = (state, action, reward, new_state)\n",
    "            #randomly sample our experience replay memory\n",
    "            minibatch = random.sample(replay, batchSize)\n",
    "            X_train = []\n",
    "            y_train = []\n",
    "            for memory in minibatch:\n",
    "                #Get max_Q(S',a)\n",
    "                old_state, action, reward, new_state = memory\n",
    "                old_qval = model.predict(old_state, batch_size=1)\n",
    "                #TODO: use target_model to predict these q values?\n",
    "                #newQ = target_model.predict(new_state, batch_size=1)\n",
    "                newQ = model.predict(new_state, batch_size=1)\n",
    "\n",
    "                maxQ = np.max(newQ)\n",
    "                y = np.zeros((1,4))\n",
    "                y[:] = old_qval[:]\n",
    "                if terminal_state == 0: #non-terminal state\n",
    "                    update = (reward + (gamma * maxQ))\n",
    "                else: #terminal state\n",
    "                    update = reward\n",
    "                y[0][action] = update\n",
    "                X_train.append(old_state)\n",
    "                y_train.append(y.reshape(4,))\n",
    "\n",
    "            X_train = np.squeeze(np.array(X_train), axis=(1))\n",
    "            y_train = np.array(y_train)\n",
    "            model.fit(X_train, y_train, batch_size=batchSize, epochs=1, verbose=0)\n",
    "            \n",
    "            if time_step % target_net_update_frequency == 0:\n",
    "                #print(\"Reseting Target Weights\")\n",
    "                weights = model.get_weights()\n",
    "                target_model.set_weights(weights)\n",
    "            #    target_weights = target_model.get_weights()\n",
    "            #    for i in xrange(len(weights)):\n",
    "            #        target_weights[i] = (1 - tau)*target_weights[i] + tau*weights[i]\n",
    "            #    target_model.set_weights(target_weights)\n",
    "            \n",
    "            state = new_state\n",
    "        steps += 1.0\n",
    "        if terminal_state == 1: #if reached terminal state, update epoch status\n",
    "            status = 0\n",
    "    avg_qval /= steps\n",
    "    eval_reward, bt = evaluate_Q(test_data, model, price_data, i)\n",
    "    #TODO save to different folder each time?\n",
    "    plot_epoch(bt, i, plt_path, test_percent=test_percent)\n",
    "    learning_progress.append((eval_reward))\n",
    "    avg_qval_list.append(avg_qval)\n",
    "    test_avg_qval_list.append(test_avg_qval)\n",
    "    print(\"Epoch: %d Reward: %f eps: %f Time: %f (s): Avg Train Q-Val: %f\" % \n",
    "          (i,eval_reward, epsilon, timeit.default_timer() - epoch_start_time, avg_qval))\n",
    "    #learning_progress.append((reward))\n",
    "    if epsilon > 0.1: #decrement epsilon over time\n",
    "        epsilon -= (1.0/epochs)\n",
    "\n",
    "#TODO save to different folder each time?\n",
    "elapsed = np.round(timeit.default_timer() - start_time, decimals=2)\n",
    "print(\"Completed in %f\" % (elapsed,))\n",
    "\n",
    "bt = twp.Backtest(pd.Series(data=[x[0,0] for x in xdata]), signal, signalType='shares')\n",
    "bt.data['delta'] = bt.data['shares'].diff().fillna(0)\n",
    "\n",
    "print(bt.data)\n",
    "unique, counts = np.unique(filter(lambda v: v==v, signal.values), return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.subplot(4,1,1)\n",
    "bt.plotTrades()\n",
    "plt.subplot(4,1,2)\n",
    "bt.pnl.plot(style='x-')\n",
    "plt.subplot(4,1,3)\n",
    "plt.plot(learning_progress)\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(avg_qval_list)\n",
    "\n",
    "plt.savefig(plt_path + '/summary'+'.png', bbox_inches='tight', pad_inches=1, dpi=72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO save model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run back test with signal which buys every time, sells every time, does nothing, does one giant buy at beginning at waits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Activation\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "class ActorNetwork(object):\n",
    "    def __init__(self, sess, state_size, action_size, BATCH_SIZE, TAU, LEARNING_RATE):\n",
    "        self.sess = sess\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.TAU = TAU\n",
    "        self.LEARNING_RATE = LEARNING_RATE\n",
    "\n",
    "        K.set_session(sess)\n",
    "\n",
    "        #Now create the model\n",
    "        self.model , self.weights, self.state = self.create_actor_network(state_size, action_size)   \n",
    "        self.target_model, self.target_weights, self.target_state = self.create_actor_network(state_size, action_size) \n",
    "        self.action_gradient = tf.placeholder(tf.float32,[None, action_size])\n",
    "        self.params_grad = tf.gradients(self.model.output, self.weights, -self.action_gradient)\n",
    "        grads = zip(self.params_grad, self.weights)\n",
    "        self.optimize = tf.train.AdamOptimizer(LEARNING_RATE).apply_gradients(grads)\n",
    "        #self.sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    def train(self, states, action_grads):\n",
    "        self.sess.run(self.optimize, feed_dict={\n",
    "            self.state: states,\n",
    "            self.action_gradient: action_grads,\n",
    "            K.learning_phase(): 1\n",
    "        })\n",
    "\n",
    "    def target_train(self):\n",
    "        actor_weights = self.model.get_weights()\n",
    "        actor_target_weights = self.target_model.get_weights()\n",
    "        for i in xrange(len(actor_weights)):\n",
    "            actor_target_weights[i] = self.TAU * actor_weights[i] + (1 - self.TAU)* actor_target_weights[i]\n",
    "        self.target_model.set_weights(actor_target_weights)\n",
    "\n",
    "    def create_actor_network(self, state_size, action_dim):\n",
    "        \n",
    "        S = Input(shape=[1, state_size])\n",
    "        L1 = LSTM(64, activation='relu', return_sequences=True,\n",
    "                       stateful=False)(S)\n",
    "        #D1 = Dropout(0.5)(L1)\n",
    "\n",
    "        L2 = LSTM(64, activation='relu', return_sequences=False,\n",
    "                       stateful=False)(L1)\n",
    "\n",
    "        #D2 = Dropout(0.5)(L2)\n",
    "        \n",
    "        FC = Dense(16, activation='relu', kernel_initializer='he_uniform')(L2)\n",
    "        O = Dense(action_dim, kernel_initializer='lecun_uniform', activation='linear')(FC) #activation??? tanh?\n",
    "        #TODO output dimension shouldn't be action_dim, should maybe have two outputs merged, \n",
    "        #one for each, possibly two FC's with both relu activations so either 0 or on, \n",
    "        #or maybe just one FC with linear acitvation and constrained to -100 to 100 range\n",
    "        model = Model(inputs=[S], outputs=[O])\n",
    "\n",
    "        return model, model.trainable_weights, S\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.layers import merge\n",
    "from keras.layers.merge import concatenate\n",
    "import keras.backend as K\n",
    "\n",
    "class CriticNetwork(object):\n",
    "    def __init__(self, sess, state_size, action_size, BATCH_SIZE, TAU, LEARNING_RATE):\n",
    "        self.sess = sess\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.TAU = TAU\n",
    "        self.LEARNING_RATE = LEARNING_RATE\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        K.set_session(sess)\n",
    "\n",
    "        #Now create the model\n",
    "        self.model, self.action, self.state = self.create_critic_network(state_size, action_size)  \n",
    "        self.target_model, self.target_action, self.target_state = self.create_critic_network(state_size, action_size)  \n",
    "        self.action_grads = tf.gradients(self.model.output, self.action)  #GRADIENTS for policy update\n",
    "        #self.sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    def gradients(self, states, actions):\n",
    "        return self.sess.run(self.action_grads, feed_dict={\n",
    "            self.state: states,\n",
    "            self.action: actions,\n",
    "            K.learning_phase(): 1\n",
    "        })[0]\n",
    "\n",
    "    def target_train(self):\n",
    "        critic_weights = self.model.get_weights()\n",
    "        critic_target_weights = self.target_model.get_weights()\n",
    "        for i in xrange(len(critic_weights)):\n",
    "            critic_target_weights[i] = self.TAU * critic_weights[i] + (1 - self.TAU)* critic_target_weights[i]\n",
    "        self.target_model.set_weights(critic_target_weights)\n",
    "\n",
    "    def create_critic_network(self, state_size,action_dim):\n",
    "        \n",
    "        \n",
    "        S = Input(shape=[1, state_size])\n",
    "        A = Input(shape=[action_dim])\n",
    "        L1 = LSTM(64, activation='relu', return_sequences=True,\n",
    "                       stateful=False)(S)\n",
    "        #D1 = Dropout(0.5)(L1)\n",
    "\n",
    "        L2 = LSTM(64, activation='relu', return_sequences=False,\n",
    "                       stateful=False)(L1)\n",
    "\n",
    "        #D2 = Dropout(0.5)(L2)\n",
    "        \n",
    "        m = concatenate([L2, A])\n",
    "        FC = Dense(16, activation='relu', kernel_initializer='he_uniform')(m)\n",
    "        O = Dense(action_dim, kernel_initializer='lecun_uniform', activation='linear')(FC) #activation??? tanh?\n",
    "        model = Model(inputs=[S, A], outputs=[O])\n",
    "        adam = Adam(lr=self.LEARNING_RATE)\n",
    "        model.compile(loss='mse', optimizer=adam)\n",
    "\n",
    "        return model, A, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.num_experiences = 0\n",
    "        self.buffer = deque()\n",
    "\n",
    "    def getBatch(self, batch_size):\n",
    "        # Randomly sample batch_size examples\n",
    "        if self.num_experiences < batch_size:\n",
    "            return random.sample(self.buffer, self.num_experiences)\n",
    "        else:\n",
    "            return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def size(self):\n",
    "        return self.buffer_size\n",
    "\n",
    "    def add(self, state, action, reward, new_state, done):\n",
    "        experience = (state, action, reward, new_state, done)\n",
    "        if self.num_experiences < self.buffer_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.num_experiences += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def count(self):\n",
    "        # if buffer is full, return buffer size\n",
    "        # otherwise, return experience counter\n",
    "        return self.num_experiences\n",
    "\n",
    "    def erase(self):\n",
    "        self.buffer = deque()\n",
    "        self.num_experiences = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Take Action\n",
    "def take_action_ddpg(state, xdata, action, signal, time_step):\n",
    "    #this should generate a list of trade signals that at evaluation time are fed to the backtester\n",
    "    #the backtester should get a list of trade signals and a list of price data for the assett\n",
    "    \n",
    "    #make necessary adjustments to state and then return it\n",
    "    time_step += 1\n",
    "    \n",
    "    #if the current iteration is the last state (\"terminal state\") then set terminal_state to 1\n",
    "    #TODO sell all stocks here?\n",
    "    #print(xdata.shape)\n",
    "    if time_step + 1 == xdata.shape[0]:\n",
    "        state = xdata[time_step-1:time_step, 0:1, :]\n",
    "        terminal_state = 1\n",
    "        #stocks_owned = np.sum(signal)\n",
    "        #signal.loc[time_step] = -stocks_owned\n",
    "        signal.loc[time_step] = 0\n",
    "\n",
    "        return state, time_step, signal, terminal_state\n",
    "\n",
    "    #move the market data window one step forward\n",
    "    state = xdata[time_step-1:time_step, 0:1, :]\n",
    "    #take action\n",
    "    if action > 0:\n",
    "        signal.loc[time_step] = 100\n",
    "    elif action < 0:\n",
    "        signal.loc[time_step] = -100\n",
    "    else:\n",
    "        signal.loc[time_step] = 0\n",
    "\n",
    "    terminal_state = 0\n",
    "\n",
    "    return state, time_step, signal, terminal_state\n",
    "\n",
    "\n",
    "def evaluate_DDPG(eval_data, eval_model, price_data, epoch=0):\n",
    "    #This function is used to evaluate the performance of the system each epoch, without the influence of epsilon and random actions\n",
    "    signal = pd.Series(index=np.arange(len(eval_data)))\n",
    "    state, xdata, price_data = init_state(eval_data)\n",
    "    status = 1\n",
    "    terminal_state = 0\n",
    "    time_step = 1\n",
    "    eval_reward = 0\n",
    "    while(status == 1):\n",
    "        #We start in state S\n",
    "        #Run the Q function on S to get predicted reward values on all the possible actions\n",
    "        action = eval_model.predict(state)[0, 0]\n",
    "        \n",
    "        #Take action, observe new state S'\n",
    "        new_state, time_step, signal, terminal_state = take_action_ddpg(state, xdata, action, signal, time_step)\n",
    "        \n",
    "        #Observe reward\n",
    "        reward, bt = get_reward(new_state, time_step, action, price_data, signal, terminal_state, eval=True, epoch=epoch)\n",
    "        eval_reward += reward\n",
    "        state = new_state\n",
    "        if terminal_state == 1: #terminal state\n",
    "            status = 0\n",
    "\n",
    "    return eval_reward, bt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#Tensorflow GPU optimization\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "BUFFER_SIZE = 200\n",
    "BATCH_SIZE = 100\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001     #Target Network HyperParameters\n",
    "LRA = 0.0001    #Learning rate for Actor\n",
    "LRC = 0.001     #Lerning rate for Critic\n",
    "\n",
    "action_dim = 1  #num of joints being controlled\n",
    "state_dim = 7  #num of features in state\n",
    "\n",
    "\n",
    "actor = ActorNetwork(sess, state_dim, action_dim, BATCH_SIZE, TAU, LRA)\n",
    "critic = CriticNetwork(sess, state_dim, action_dim, BATCH_SIZE, TAU, LRC)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actor.modelh = 'plt_ddpg'\n",
    "\n",
    "if not os.path.exists(plt_path):\n",
    "    os.makedirs(plt_path)\n",
    "\n",
    "\n",
    "actor.model.save_weights(plt_path + \"/actor.h5\")\n",
    "actor.target_model.save_weights(plt_path + \"/actor_target.h5\")\n",
    "\n",
    "critic.model.save_weights(plt_path + \"/critic.h5\")\n",
    "critic.target_model.save_weights(plt_path + \"/critic_target.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symbol = 'CENX'\n",
    "#TODO run with other increasing stocks (AAPL, GOOG, GNC, etc.)\n",
    "#TODO run with NIHD stock (decreasing over time, AMD?, etc) and only 1000 days\n",
    "#TODO run with cyclycal stock (UNP?, WLK?, CENX?, etc.)\n",
    "read_convert_data(symbol=symbol) #run once to read indata, resample and convert to pickle (if doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_percent = 0.15\n",
    "#max_data = 1825 #5 years (minus leap years)\n",
    "max_data = 1500 # 3 years\n",
    "\n",
    "train_data = load_data(test=False, symbol=symbol, max_data=max_data, test_percent=test_percent)\n",
    "test_data = load_data(test=True, symbol=symbol, max_data=max_data, test_percent=test_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAEyCAYAAADeAVWKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd0a82f4b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = test_data['close'].values\n",
    "plt.figure(figsize=(10,5)) \n",
    "plt.plot(d)\n",
    "split = len(test_data) - int(len(test_data)*test_percent)\n",
    "plt.axvline(x=split, color='black', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLORE = 150.0\n",
    "init_epsilon = 0.5\n",
    "epochs = 250\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Total Reward: -1636.000000 Eval Reward: 459.000000 eps: 0.500000 Time: 123.321589 (s)\n",
      "Epoch: 1 Total Reward: -764.640000 Eval Reward: 459.000000 eps: 0.496667 Time: 79.988645 (s)\n",
      "Epoch: 2 Total Reward: -1288.500000 Eval Reward: 459.000000 eps: 0.493333 Time: 79.801341 (s)\n",
      "Epoch: 3 Total Reward: 1614.700000 Eval Reward: 459.000000 eps: 0.490000 Time: 81.225581 (s)\n",
      "Epoch: 4 Total Reward: 986.600000 Eval Reward: 459.000000 eps: 0.486667 Time: 81.730055 (s)\n",
      "Epoch: 5 Total Reward: 2677.880000 Eval Reward: 459.000000 eps: 0.483333 Time: 83.423892 (s)\n",
      "Epoch: 6 Total Reward: -454.400000 Eval Reward: 459.000000 eps: 0.480000 Time: 81.249903 (s)\n",
      "Epoch: 7 Total Reward: -312.530000 Eval Reward: 713.000000 eps: 0.476667 Time: 81.269494 (s)\n",
      "Epoch: 8 Total Reward: 1325.690000 Eval Reward: -331.000000 eps: 0.473333 Time: 81.939273 (s)\n",
      "Epoch: 9 Total Reward: 118.640000 Eval Reward: -459.000000 eps: 0.470000 Time: 81.458334 (s)\n",
      "Epoch: 10 Total Reward: 512.590000 Eval Reward: 2113.200000 eps: 0.466667 Time: 81.122615 (s)\n",
      "Epoch: 11 Total Reward: -1080.000000 Eval Reward: -459.000000 eps: 0.463333 Time: 81.774573 (s)\n",
      "Epoch: 12 Total Reward: 92.460000 Eval Reward: -264.020000 eps: 0.460000 Time: 82.488840 (s)\n",
      "Epoch: 13 Total Reward: 664.320000 Eval Reward: -459.000000 eps: 0.456667 Time: 81.871477 (s)\n",
      "Epoch: 14 Total Reward: 237.820000 Eval Reward: -137.700000 eps: 0.453333 Time: 83.470876 (s)\n",
      "Epoch: 15 Total Reward: 1396.670000 Eval Reward: 1718.200000 eps: 0.450000 Time: 77.844797 (s)\n",
      "Epoch: 16 Total Reward: -319.520000 Eval Reward: 652.000000 eps: 0.446667 Time: 76.809682 (s)\n",
      "Epoch: 17 Total Reward: 719.230000 Eval Reward: 330.000000 eps: 0.443333 Time: 78.316497 (s)\n",
      "Epoch: 18 Total Reward: 620.090000 Eval Reward: 165.000000 eps: 0.440000 Time: 76.779992 (s)\n",
      "Epoch: 19 Total Reward: -169.020000 Eval Reward: 273.000000 eps: 0.436667 Time: 77.524229 (s)\n",
      "Epoch: 20 Total Reward: 739.970000 Eval Reward: 566.000000 eps: 0.433333 Time: 71.619627 (s)\n",
      "Epoch: 21 Total Reward: -767.540000 Eval Reward: 635.000000 eps: 0.430000 Time: 72.269012 (s)\n",
      "Epoch: 22 Total Reward: 485.390000 Eval Reward: 995.000000 eps: 0.426667 Time: 71.508825 (s)\n",
      "Epoch: 23 Total Reward: -1073.020000 Eval Reward: 41.000000 eps: 0.423333 Time: 71.672139 (s)\n",
      "Epoch: 24 Total Reward: -470.700000 Eval Reward: 1850.000000 eps: 0.420000 Time: 71.738952 (s)\n",
      "Epoch: 25 Total Reward: 1423.650000 Eval Reward: 1322.000000 eps: 0.416667 Time: 71.819273 (s)\n",
      "Epoch: 26 Total Reward: 20.110000 Eval Reward: 120.000000 eps: 0.413333 Time: 70.935920 (s)\n",
      "Epoch: 27 Total Reward: 764.380000 Eval Reward: 575.000000 eps: 0.410000 Time: 71.248409 (s)\n",
      "Epoch: 28 Total Reward: -1030.490000 Eval Reward: 459.000000 eps: 0.406667 Time: 72.232958 (s)\n",
      "Epoch: 29 Total Reward: 431.990000 Eval Reward: 459.000000 eps: 0.403333 Time: 71.544840 (s)\n",
      "Epoch: 30 Total Reward: 371.960000 Eval Reward: 669.000000 eps: 0.400000 Time: 70.684712 (s)\n",
      "Epoch: 31 Total Reward: 1874.040000 Eval Reward: 669.000000 eps: 0.396667 Time: 72.060442 (s)\n",
      "Epoch: 32 Total Reward: -201.800000 Eval Reward: 493.000000 eps: 0.393333 Time: 71.211125 (s)\n",
      "Epoch: 33 Total Reward: -77.730000 Eval Reward: 250.000000 eps: 0.390000 Time: 71.076018 (s)\n",
      "Epoch: 34 Total Reward: 1694.490000 Eval Reward: 443.000000 eps: 0.386667 Time: 71.969203 (s)\n",
      "Epoch: 35 Total Reward: 1203.990000 Eval Reward: 443.000000 eps: 0.383333 Time: 71.800915 (s)\n",
      "Epoch: 36 Total Reward: 195.080000 Eval Reward: 433.000000 eps: 0.380000 Time: 70.757547 (s)\n",
      "Epoch: 37 Total Reward: 605.680000 Eval Reward: -285.000000 eps: 0.376667 Time: 71.943317 (s)\n",
      "Epoch: 38 Total Reward: 607.660000 Eval Reward: 1613.000000 eps: 0.373333 Time: 72.096878 (s)\n",
      "Epoch: 39 Total Reward: -15.500000 Eval Reward: 1117.000000 eps: 0.370000 Time: 71.016874 (s)\n",
      "Epoch: 40 Total Reward: -792.490000 Eval Reward: 263.000000 eps: 0.366667 Time: 71.281964 (s)\n",
      "Epoch: 41 Total Reward: 178.460000 Eval Reward: 601.660000 eps: 0.363333 Time: 72.082767 (s)\n",
      "Epoch: 42 Total Reward: -1553.430000 Eval Reward: -771.320000 eps: 0.360000 Time: 71.305507 (s)\n",
      "Epoch: 43 Total Reward: -1337.140000 Eval Reward: 44.700000 eps: 0.356667 Time: 70.973495 (s)\n",
      "Epoch: 44 Total Reward: -1412.340000 Eval Reward: 1170.020000 eps: 0.353333 Time: 72.306414 (s)\n",
      "Epoch: 45 Total Reward: -175.730000 Eval Reward: -649.320000 eps: 0.350000 Time: 71.633286 (s)\n",
      "Epoch: 46 Total Reward: 1289.030000 Eval Reward: -328.980000 eps: 0.346667 Time: 71.065603 (s)\n",
      "Epoch: 47 Total Reward: -432.460000 Eval Reward: 3177.000000 eps: 0.343333 Time: 71.922585 (s)\n",
      "Epoch: 48 Total Reward: 51.880000 Eval Reward: 2638.320000 eps: 0.340000 Time: 72.164573 (s)\n",
      "Epoch: 49 Total Reward: 610.000000 Eval Reward: -218.000000 eps: 0.336667 Time: 70.552175 (s)\n",
      "Epoch: 50 Total Reward: 893.860000 Eval Reward: -2660.000000 eps: 0.333333 Time: 71.883561 (s)\n",
      "Epoch: 51 Total Reward: -582.470000 Eval Reward: 47.000000 eps: 0.330000 Time: 72.308721 (s)\n",
      "Epoch: 52 Total Reward: -458.960000 Eval Reward: -1744.980000 eps: 0.326667 Time: 71.106687 (s)\n",
      "Epoch: 53 Total Reward: -790.150000 Eval Reward: -1422.000000 eps: 0.323333 Time: 71.786060 (s)\n",
      "Epoch: 54 Total Reward: 703.470000 Eval Reward: -1998.000000 eps: 0.320000 Time: 72.258929 (s)\n",
      "Epoch: 55 Total Reward: -1188.820000 Eval Reward: -2956.980000 eps: 0.316667 Time: 71.360579 (s)\n",
      "Epoch: 56 Total Reward: 196.500000 Eval Reward: -3481.000000 eps: 0.313333 Time: 71.338861 (s)\n",
      "Epoch: 57 Total Reward: -155.900000 Eval Reward: -579.000000 eps: 0.310000 Time: 73.762052 (s)\n",
      "Epoch: 58 Total Reward: 220.340000 Eval Reward: -657.000000 eps: 0.306667 Time: 71.806461 (s)\n",
      "Epoch: 59 Total Reward: -639.680000 Eval Reward: -1775.000000 eps: 0.303333 Time: 71.819930 (s)\n",
      "Epoch: 60 Total Reward: -966.900000 Eval Reward: -492.980000 eps: 0.300000 Time: 72.281506 (s)\n",
      "Epoch: 61 Total Reward: 643.500000 Eval Reward: 436.000000 eps: 0.296667 Time: 72.222224 (s)\n",
      "Epoch: 62 Total Reward: 126.440000 Eval Reward: -728.980000 eps: 0.293333 Time: 70.900856 (s)\n",
      "Epoch: 63 Total Reward: 712.500000 Eval Reward: -457.980000 eps: 0.290000 Time: 71.891563 (s)\n",
      "Epoch: 64 Total Reward: -843.900000 Eval Reward: 2078.020000 eps: 0.286667 Time: 72.005975 (s)\n",
      "Epoch: 65 Total Reward: -98.180000 Eval Reward: -913.000000 eps: 0.283333 Time: 71.109430 (s)\n",
      "Epoch: 66 Total Reward: -171.080000 Eval Reward: -2065.000000 eps: 0.280000 Time: 72.067904 (s)\n",
      "Epoch: 67 Total Reward: 531.840000 Eval Reward: 402.000000 eps: 0.276667 Time: 72.174153 (s)\n",
      "Epoch: 68 Total Reward: -1694.500000 Eval Reward: -1410.980000 eps: 0.273333 Time: 71.293870 (s)\n",
      "Epoch: 69 Total Reward: 225.310000 Eval Reward: -3878.000000 eps: 0.270000 Time: 72.137613 (s)\n",
      "Epoch: 70 Total Reward: 1303.320000 Eval Reward: -3458.000000 eps: 0.266667 Time: 72.322378 (s)\n",
      "Epoch: 71 Total Reward: 156.620000 Eval Reward: -2143.000000 eps: 0.263333 Time: 71.273459 (s)\n",
      "Epoch: 72 Total Reward: -955.020000 Eval Reward: -3014.000000 eps: 0.260000 Time: 71.805096 (s)\n",
      "Epoch: 73 Total Reward: 772.000000 Eval Reward: -3978.980000 eps: 0.256667 Time: 72.513903 (s)\n",
      "Epoch: 74 Total Reward: 1517.790000 Eval Reward: -3068.000000 eps: 0.253333 Time: 71.163516 (s)\n",
      "Epoch: 75 Total Reward: -26.660000 Eval Reward: -599.000000 eps: 0.250000 Time: 71.286332 (s)\n",
      "Epoch: 76 Total Reward: -189.170000 Eval Reward: -1400.980000 eps: 0.246667 Time: 72.484056 (s)\n",
      "Epoch: 77 Total Reward: 641.170000 Eval Reward: -2064.980000 eps: 0.243333 Time: 71.668351 (s)\n",
      "Epoch: 78 Total Reward: 14.370000 Eval Reward: 741.000000 eps: 0.240000 Time: 71.228038 (s)\n",
      "Epoch: 79 Total Reward: 63.860000 Eval Reward: -2518.980000 eps: 0.236667 Time: 71.926462 (s)\n",
      "Epoch: 80 Total Reward: -655.660000 Eval Reward: -2837.980000 eps: 0.233333 Time: 71.486978 (s)\n",
      "Epoch: 81 Total Reward: 1753.160000 Eval Reward: -959.980000 eps: 0.230000 Time: 70.906655 (s)\n",
      "Epoch: 82 Total Reward: -873.390000 Eval Reward: -925.000000 eps: 0.226667 Time: 72.438596 (s)\n",
      "Epoch: 83 Total Reward: -804.860000 Eval Reward: -961.000000 eps: 0.223333 Time: 72.055493 (s)\n",
      "Epoch: 84 Total Reward: 823.490000 Eval Reward: 30.000000 eps: 0.220000 Time: 70.976233 (s)\n",
      "Epoch: 85 Total Reward: 145.840000 Eval Reward: -839.000000 eps: 0.216667 Time: 72.008554 (s)\n",
      "Epoch: 86 Total Reward: 769.500000 Eval Reward: -3131.000000 eps: 0.213333 Time: 72.666415 (s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87 Total Reward: -167.900000 Eval Reward: -2915.000000 eps: 0.210000 Time: 71.027025 (s)\n",
      "Epoch: 88 Total Reward: -462.500000 Eval Reward: -1932.000000 eps: 0.206667 Time: 72.046801 (s)\n",
      "Epoch: 89 Total Reward: 340.830000 Eval Reward: -2910.000000 eps: 0.203333 Time: 72.392251 (s)\n",
      "Epoch: 90 Total Reward: 325.780000 Eval Reward: -3302.000000 eps: 0.200000 Time: 71.527282 (s)\n",
      "Epoch: 91 Total Reward: -1031.670000 Eval Reward: -1419.980000 eps: 0.196667 Time: 71.810633 (s)\n",
      "Epoch: 92 Total Reward: 269.010000 Eval Reward: -1251.980000 eps: 0.193333 Time: 72.101305 (s)\n",
      "Epoch: 93 Total Reward: 223.500000 Eval Reward: -1387.980000 eps: 0.190000 Time: 71.237471 (s)\n",
      "Epoch: 94 Total Reward: 1016.800000 Eval Reward: -959.980000 eps: 0.186667 Time: 71.907361 (s)\n",
      "Epoch: 95 Total Reward: 193.000000 Eval Reward: -1367.980000 eps: 0.183333 Time: 72.234397 (s)\n",
      "Epoch: 96 Total Reward: -232.980000 Eval Reward: -1507.980000 eps: 0.180000 Time: 71.409321 (s)\n",
      "Epoch: 97 Total Reward: 475.500000 Eval Reward: -1435.980000 eps: 0.176667 Time: 71.200908 (s)\n",
      "Epoch: 98 Total Reward: 1116.010000 Eval Reward: -1403.980000 eps: 0.173333 Time: 72.499266 (s)\n",
      "Epoch: 99 Total Reward: 166.920000 Eval Reward: -1267.980000 eps: 0.170000 Time: 71.515049 (s)\n",
      "Epoch: 100 Total Reward: -25.680000 Eval Reward: -1062.000000 eps: 0.166667 Time: 71.182024 (s)\n",
      "Epoch: 101 Total Reward: 724.430000 Eval Reward: -1516.980000 eps: 0.163333 Time: 72.321643 (s)\n",
      "Epoch: 102 Total Reward: 426.500000 Eval Reward: -1500.980000 eps: 0.160000 Time: 71.699288 (s)\n",
      "Epoch: 103 Total Reward: 755.630000 Eval Reward: -1484.980000 eps: 0.156667 Time: 71.540046 (s)\n",
      "Epoch: 104 Total Reward: 721.480000 Eval Reward: -1372.980000 eps: 0.153333 Time: 72.366578 (s)\n",
      "Epoch: 105 Total Reward: -494.490000 Eval Reward: -1362.980000 eps: 0.150000 Time: 72.275412 (s)\n",
      "Epoch: 106 Total Reward: -485.600000 Eval Reward: -1025.980000 eps: 0.146667 Time: 71.081122 (s)\n",
      "Epoch: 107 Total Reward: 212.990000 Eval Reward: -925.980000 eps: 0.143333 Time: 72.147701 (s)\n",
      "Epoch: 108 Total Reward: 503.850000 Eval Reward: -1549.980000 eps: 0.140000 Time: 72.229896 (s)\n",
      "Epoch: 109 Total Reward: -571.500000 Eval Reward: -2113.980000 eps: 0.136667 Time: 71.085003 (s)\n",
      "Epoch: 110 Total Reward: 871.480000 Eval Reward: -1695.000000 eps: 0.133333 Time: 71.618562 (s)\n",
      "Epoch: 111 Total Reward: -721.130000 Eval Reward: -379.000000 eps: 0.130000 Time: 72.852790 (s)\n",
      "Epoch: 112 Total Reward: 697.350000 Eval Reward: -672.000000 eps: 0.126667 Time: 71.523260 (s)\n",
      "Epoch: 113 Total Reward: -609.650000 Eval Reward: -506.000000 eps: 0.123333 Time: 71.843140 (s)\n",
      "Epoch: 114 Total Reward: 721.340000 Eval Reward: -956.000000 eps: 0.120000 Time: 73.834918 (s)\n",
      "Epoch: 115 Total Reward: -214.660000 Eval Reward: -565.000000 eps: 0.116667 Time: 71.760569 (s)\n",
      "Epoch: 116 Total Reward: 427.840000 Eval Reward: -380.680000 eps: 0.113333 Time: 72.319663 (s)\n",
      "Epoch: 117 Total Reward: 1138.340000 Eval Reward: -518.980000 eps: 0.110000 Time: 72.463707 (s)\n",
      "Epoch: 118 Total Reward: 333.680000 Eval Reward: -133.680000 eps: 0.106667 Time: 71.933083 (s)\n",
      "Epoch: 119 Total Reward: 784.520000 Eval Reward: -67.680000 eps: 0.103333 Time: 71.946303 (s)\n",
      "Epoch: 120 Total Reward: 147.840000 Eval Reward: -318.000000 eps: 0.100000 Time: 72.619267 (s)\n",
      "Epoch: 121 Total Reward: -674.650000 Eval Reward: -319.680000 eps: 0.096667 Time: 71.798176 (s)\n",
      "Epoch: 122 Total Reward: -435.150000 Eval Reward: -108.000000 eps: 0.093333 Time: 71.900465 (s)\n",
      "Epoch: 123 Total Reward: 1066.340000 Eval Reward: -212.680000 eps: 0.090000 Time: 73.162738 (s)\n",
      "Epoch: 124 Total Reward: 141.830000 Eval Reward: -109.680000 eps: 0.086667 Time: 71.320547 (s)\n",
      "Epoch: 125 Total Reward: 337.660000 Eval Reward: -379.660000 eps: 0.083333 Time: 72.034366 (s)\n",
      "Epoch: 126 Total Reward: -183.010000 Eval Reward: -321.660000 eps: 0.080000 Time: 73.181727 (s)\n",
      "Epoch: 127 Total Reward: -348.000000 Eval Reward: -152.980000 eps: 0.076667 Time: 73.134585 (s)\n",
      "Epoch: 128 Total Reward: -63.500000 Eval Reward: -460.980000 eps: 0.073333 Time: 72.451407 (s)\n",
      "Epoch: 129 Total Reward: 341.510000 Eval Reward: -881.980000 eps: 0.070000 Time: 71.362641 (s)\n",
      "Epoch: 130 Total Reward: 89.840000 Eval Reward: -733.980000 eps: 0.066667 Time: 72.907728 (s)\n",
      "Epoch: 131 Total Reward: -5.000000 Eval Reward: -653.980000 eps: 0.063333 Time: 72.263980 (s)\n",
      "Epoch: 132 Total Reward: -337.500000 Eval Reward: -331.980000 eps: 0.060000 Time: 71.469522 (s)\n",
      "Epoch: 133 Total Reward: -134.500000 Eval Reward: -684.980000 eps: 0.056667 Time: 72.628552 (s)\n",
      "Epoch: 134 Total Reward: 1133.000000 Eval Reward: -656.980000 eps: 0.053333 Time: 72.070960 (s)\n",
      "Epoch: 135 Total Reward: 1114.600000 Eval Reward: -892.980000 eps: 0.050000 Time: 71.597612 (s)\n",
      "Epoch: 136 Total Reward: 547.980000 Eval Reward: -954.980000 eps: 0.046667 Time: 72.821895 (s)\n",
      "Epoch: 137 Total Reward: 299.500000 Eval Reward: -928.980000 eps: 0.043333 Time: 103.860119 (s)\n",
      "Epoch: 138 Total Reward: -595.500000 Eval Reward: -906.980000 eps: 0.040000 Time: 106.652527 (s)\n",
      "Epoch: 139 Total Reward: 454.540000 Eval Reward: -1441.000000 eps: 0.036667 Time: 86.577813 (s)\n",
      "Epoch: 140 Total Reward: 542.840000 Eval Reward: 296.980000 eps: 0.033333 Time: 83.849202 (s)\n",
      "Epoch: 141 Total Reward: -406.660000 Eval Reward: -2166.020000 eps: 0.030000 Time: 83.547882 (s)\n",
      "Epoch: 142 Total Reward: -304.500000 Eval Reward: -1957.000000 eps: 0.026667 Time: 83.955355 (s)\n",
      "Epoch: 143 Total Reward: 1219.500000 Eval Reward: -2755.000000 eps: 0.023333 Time: 83.451807 (s)\n",
      "Epoch: 144 Total Reward: -83.000000 Eval Reward: -393.000000 eps: 0.020000 Time: 84.618118 (s)\n",
      "Epoch: 145 Total Reward: -68.010000 Eval Reward: 505.000000 eps: 0.016667 Time: 83.331170 (s)\n",
      "Epoch: 146 Total Reward: 95.500000 Eval Reward: 561.000000 eps: 0.013333 Time: 83.114030 (s)\n",
      "Epoch: 147 Total Reward: 562.000000 Eval Reward: -1663.000000 eps: 0.010000 Time: 83.274083 (s)\n",
      "Epoch: 148 Total Reward: -717.000000 Eval Reward: -4198.000000 eps: 0.006667 Time: 83.007239 (s)\n",
      "Epoch: 149 Total Reward: 876.500000 Eval Reward: 535.000000 eps: 0.003333 Time: 82.511459 (s)\n",
      "Epoch: 150 Total Reward: -171.500000 Eval Reward: -1377.000000 eps: -0.000000 Time: 83.440981 (s)\n",
      "Epoch: 151 Total Reward: -110.000000 Eval Reward: -726.000000 eps: -0.003333 Time: 83.330406 (s)\n",
      "Epoch: 152 Total Reward: -81.000000 Eval Reward: -292.000000 eps: -0.006667 Time: 83.695315 (s)\n",
      "Epoch: 153 Total Reward: -944.500000 Eval Reward: -554.000000 eps: -0.010000 Time: 84.237279 (s)\n",
      "Epoch: 154 Total Reward: -922.500000 Eval Reward: -903.000000 eps: -0.013333 Time: 83.315996 (s)\n",
      "Epoch: 155 Total Reward: -423.020000 Eval Reward: -683.000000 eps: -0.016667 Time: 82.937951 (s)\n",
      "Epoch: 156 Total Reward: 219.500000 Eval Reward: -210.000000 eps: -0.020000 Time: 83.116892 (s)\n",
      "Epoch: 157 Total Reward: -317.000000 Eval Reward: 2084.000000 eps: -0.023333 Time: 83.200849 (s)\n",
      "Epoch: 158 Total Reward: -1223.500000 Eval Reward: -1873.980000 eps: -0.026667 Time: 83.492434 (s)\n",
      "Epoch: 159 Total Reward: 639.000000 Eval Reward: -639.980000 eps: -0.030000 Time: 82.160094 (s)\n",
      "Epoch: 160 Total Reward: -445.500000 Eval Reward: -589.000000 eps: -0.033333 Time: 85.657105 (s)\n",
      "Epoch: 161 Total Reward: 164.500000 Eval Reward: -741.000000 eps: -0.036667 Time: 84.428548 (s)\n",
      "Epoch: 162 Total Reward: 136.000000 Eval Reward: -976.000000 eps: -0.040000 Time: 83.871268 (s)\n",
      "Epoch: 163 Total Reward: 6.500000 Eval Reward: -893.000000 eps: -0.043333 Time: 83.907326 (s)\n",
      "Epoch: 164 Total Reward: 225.500000 Eval Reward: -260.000000 eps: -0.046667 Time: 81.429230 (s)\n",
      "Epoch: 165 Total Reward: -313.000000 Eval Reward: -599.000000 eps: -0.050000 Time: 79.061346 (s)\n",
      "Epoch: 166 Total Reward: -420.000000 Eval Reward: 336.020000 eps: -0.053333 Time: 78.671657 (s)\n",
      "Epoch: 167 Total Reward: -145.500000 Eval Reward: -718.000000 eps: -0.056667 Time: 78.814452 (s)\n",
      "Epoch: 168 Total Reward: 121.500000 Eval Reward: 383.000000 eps: -0.060000 Time: 79.018483 (s)\n",
      "Epoch: 169 Total Reward: -285.500000 Eval Reward: -43.000000 eps: -0.063333 Time: 78.464290 (s)\n",
      "Epoch: 170 Total Reward: 74.500000 Eval Reward: -62.000000 eps: -0.066667 Time: 84.802450 (s)\n",
      "Epoch: 171 Total Reward: -251.500000 Eval Reward: -90.000000 eps: -0.070000 Time: 89.536206 (s)\n",
      "Epoch: 172 Total Reward: 60.000000 Eval Reward: 564.000000 eps: -0.073333 Time: 83.490068 (s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 173 Total Reward: 233.000000 Eval Reward: 1695.000000 eps: -0.076667 Time: 88.825527 (s)\n",
      "Epoch: 174 Total Reward: -226.500000 Eval Reward: 991.000000 eps: -0.080000 Time: 83.517928 (s)\n",
      "Epoch: 175 Total Reward: 361.500000 Eval Reward: 1523.000000 eps: -0.083333 Time: 83.803946 (s)\n",
      "Epoch: 176 Total Reward: 220.000000 Eval Reward: 623.000000 eps: -0.086667 Time: 83.539168 (s)\n",
      "Epoch: 177 Total Reward: -183.500000 Eval Reward: -1317.220000 eps: -0.090000 Time: 83.755758 (s)\n",
      "Epoch: 178 Total Reward: -7.380000 Eval Reward: -1138.180000 eps: -0.093333 Time: 83.579735 (s)\n",
      "Epoch: 179 Total Reward: 190.950000 Eval Reward: 18.120000 eps: -0.096667 Time: 84.010418 (s)\n",
      "Epoch: 180 Total Reward: -2.510000 Eval Reward: 59.320000 eps: -0.100000 Time: 83.842933 (s)\n",
      "Epoch: 181 Total Reward: -12.520000 Eval Reward: 736.020000 eps: -0.103333 Time: 84.748375 (s)\n",
      "Epoch: 182 Total Reward: -78.500000 Eval Reward: 140.000000 eps: -0.106667 Time: 83.928292 (s)\n",
      "Epoch: 183 Total Reward: 97.500000 Eval Reward: 459.000000 eps: -0.110000 Time: 83.555386 (s)\n",
      "Epoch: 184 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.113333 Time: 83.659346 (s)\n",
      "Epoch: 185 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.116667 Time: 84.756785 (s)\n",
      "Epoch: 186 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.120000 Time: 83.879333 (s)\n",
      "Epoch: 187 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.123333 Time: 83.861322 (s)\n",
      "Epoch: 188 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.126667 Time: 83.672790 (s)\n",
      "Epoch: 189 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.130000 Time: 83.958368 (s)\n",
      "Epoch: 190 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.133333 Time: 83.033487 (s)\n",
      "Epoch: 191 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.136667 Time: 79.380247 (s)\n",
      "Epoch: 192 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.140000 Time: 78.387036 (s)\n",
      "Epoch: 193 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.143333 Time: 79.334420 (s)\n",
      "Epoch: 194 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.146667 Time: 78.681078 (s)\n",
      "Epoch: 195 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.150000 Time: 78.870891 (s)\n",
      "Epoch: 196 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.153333 Time: 78.900960 (s)\n",
      "Epoch: 197 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.156667 Time: 79.222657 (s)\n",
      "Epoch: 198 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.160000 Time: 79.023903 (s)\n",
      "Epoch: 199 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.163333 Time: 88.943390 (s)\n",
      "Epoch: 200 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.166667 Time: 83.489752 (s)\n",
      "Epoch: 201 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.170000 Time: 84.216608 (s)\n",
      "Epoch: 202 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.173333 Time: 85.970976 (s)\n",
      "Epoch: 203 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.176667 Time: 83.061327 (s)\n",
      "Epoch: 204 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.180000 Time: 82.887653 (s)\n",
      "Epoch: 205 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.183333 Time: 92.592336 (s)\n",
      "Epoch: 206 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.186667 Time: 92.818049 (s)\n",
      "Epoch: 207 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.190000 Time: 566.764818 (s)\n",
      "Epoch: 208 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.193333 Time: 82.542643 (s)\n",
      "Epoch: 209 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.196667 Time: 85.715412 (s)\n",
      "Epoch: 210 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.200000 Time: 83.375371 (s)\n",
      "Epoch: 211 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.203333 Time: 85.649233 (s)\n",
      "Epoch: 212 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.206667 Time: 87.404624 (s)\n",
      "Epoch: 213 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.210000 Time: 81.757461 (s)\n",
      "Epoch: 214 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.213333 Time: 80.985151 (s)\n",
      "Epoch: 215 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.216667 Time: 81.033388 (s)\n",
      "Epoch: 216 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.220000 Time: 81.391174 (s)\n",
      "Epoch: 217 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.223333 Time: 85.240500 (s)\n",
      "Epoch: 218 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.226667 Time: 83.014124 (s)\n",
      "Epoch: 219 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.230000 Time: 81.399122 (s)\n",
      "Epoch: 220 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.233333 Time: 84.279867 (s)\n",
      "Epoch: 221 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.236667 Time: 81.259703 (s)\n",
      "Epoch: 222 Total Reward: 46.000000 Eval Reward: 459.000000 eps: -0.240000 Time: 82.693752 (s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception KeyboardInterrupt: KeyboardInterrupt() in <bound method Session.__del__ of <tensorflow.python.client.session.Session object at 0x7fd0da9d7b90>> ignored\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-500-715862937a86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_for_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-436-532e5bce0e51>\u001b[0m in \u001b[0;36mtarget_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mactor_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mactor_target_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mactor_target_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTAU\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mactor_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTAU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mactor_target_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/.conda/envs/proj/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mget_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1986\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1987\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1988\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1990\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/.conda/envs/proj/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(ops)\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \"\"\"\n\u001b[1;32m   2142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2144\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/.conda/envs/proj/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/.conda/envs/proj/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m()\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0muninitialized_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0muninitialized_variables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "signal = pd.Series(index=np.arange(len(train_data)))\n",
    "learning_progress = []\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "    \n",
    "buff = ReplayBuffer(BUFFER_SIZE)    #Create replay buffer\n",
    "epsilon = init_epsilon\n",
    "\n",
    "for i in range(epochs): #change to while not done as above\n",
    "\n",
    "    \n",
    "    epoch_start_time = timeit.default_timer()\n",
    "\n",
    "    if i == epochs-1: #the last epoch, use test data set\n",
    "        state, xdata, price_data = init_state(test_data, test=True)\n",
    "    else:\n",
    "        state, xdata, price_data = init_state(train_data)\n",
    "    status = 1\n",
    "    terminal_state = 0\n",
    "    time_step = 14\n",
    "    total_reward = 0\n",
    "    \n",
    "    while(status == 1):\n",
    "        \n",
    "        action = np.zeros([1,action_dim])\n",
    "        #noise_t = np.zeros([1,action_dim])\n",
    "        \n",
    "\n",
    "        if np.random.random() > epsilon:\n",
    "            action = actor.model.predict(state)*1 #rescale\n",
    "        else:\n",
    "            action = np.random.uniform(-100,100, size=(1,1))\n",
    "            \n",
    "            \n",
    "        #Take action, observe new state S'\n",
    "        #TODO change up take_action to take 1 value that is pos or neg\n",
    "        new_state, time_step, signal, terminal_state = take_action_ddpg(state, xdata, action[0, 0], signal, time_step)\n",
    "        #Observe reward\n",
    "        reward, bt = get_reward(new_state, time_step, action, price_data, signal, terminal_state)\n",
    "        total_reward += reward\n",
    "\n",
    "        buff.add(state, action[0], reward, new_state, terminal_state)      #Add replay buffer\n",
    "\n",
    "        #Do the batch update\n",
    "        batch = buff.getBatch(BATCH_SIZE)\n",
    "        states = np.asarray([e[0] for e in batch])[:, 0, :, :]\n",
    "        actions = np.asarray([e[1] for e in batch])\n",
    "        rewards = np.asarray([e[2] for e in batch])\n",
    "        new_states = np.asarray([e[3] for e in batch])[:, 0, :, :]\n",
    "        dones = np.asarray([e[4] for e in batch])\n",
    "        y_t = np.asarray([e[1] for e in batch])\n",
    "        \n",
    "        target_q_values = critic.target_model.predict([new_states, actor.target_model.predict(new_states)])  \n",
    "\n",
    "        for k in range(len(batch)):\n",
    "            if dones[k]:\n",
    "                y_t[k] = rewards[k]\n",
    "            else:\n",
    "                y_t[k] = rewards[k] + GAMMA*target_q_values[k]\n",
    "\n",
    "        critic.model.train_on_batch([states,actions], y_t) \n",
    "        a_for_grad = actor.model.predict(states)\n",
    "        grads = critic.gradients(states, a_for_grad)\n",
    "        actor.train(states, grads)\n",
    "        actor.target_train()\n",
    "        critic.target_train()\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        if terminal_state == 1: #if reached terminal state, update epoch status\n",
    "            status = 0\n",
    "\n",
    "    #if np.mod(i, 3) == 0:\n",
    "    #    if (train_indicator):\n",
    "    #        actor.model.save_weights(\"actormodel.h5\", overwrite=True)\n",
    "    #        critic.model.save_weights(\"criticmodel.h5\", overwrite=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    eval_reward, bt = evaluate_DDPG(test_data, actor.model, price_data, i)\n",
    "    plot_epoch(bt, i, plt_path, test_percent=test_percent)\n",
    "    learning_progress.append((eval_reward))\n",
    "    print(\"Epoch: %s Total Reward: %f Eval Reward: %f eps: %f Time: %f (s)\" % \n",
    "          (i, total_reward, eval_reward, epsilon, timeit.default_timer() - epoch_start_time))\n",
    "    \n",
    "    epsilon -= init_epsilon / EXPLORE\n",
    "    #if epsilon < 0.1:\n",
    "    #    epsilon = 0.1\n",
    "\n",
    "\n",
    "#TODO other end of game stuffelapsed = np.round(timeit.default_timer() - start_time, decimals=2)\n",
    "print(\"Completed in %f\" % (elapsed,))\n",
    "\n",
    "bt = twp.Backtest(pd.Series(data=[x[0,0] for x in xdata]), signal, signalType='shares')\n",
    "bt.data['delta'] = bt.data['shares'].diff().fillna(0)\n",
    "\n",
    "print(bt.data)\n",
    "unique, counts = np.unique(filter(lambda v: v==v, signal.values), return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.subplot(3,1,1)\n",
    "bt.plotTrades()\n",
    "plt.subplot(3,1,2)\n",
    "bt.pnl.plot(style='x-')\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(learning_progress)\n",
    "\n",
    "plt.savefig(plt_path + '/summary'+'.png', bbox_inches='tight', pad_inches=1, dpi=72)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO fixup for stock trading\n",
    "\n",
    "# A2C(Advantage Actor-Critic) agent for the Cartpole\n",
    "class A2CAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.value_size = 1\n",
    "\n",
    "        # These are hyper parameters for the Policy Gradient\n",
    "        self.discount_factor = 0.99\n",
    "        self.actor_lr = 0.001\n",
    "        self.critic_lr = 0.005\n",
    "\n",
    "        # create model for policy network\n",
    "        self.actor = self.build_actor()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.actor.load_weights(\"./save_model/cartpole_actor.h5\")\n",
    "            self.critic.load_weights(\"./save_model/cartpole_critic.h5\")\n",
    "            \n",
    "    def build_model(self):\n",
    "        #TODO build one model with state input and 2 outputs (policy [size of action space] and value [size 1])\n",
    "\n",
    "    # approximate policy and value using Neural Network\n",
    "    # actor: state is input and probability of each action is output of model\n",
    "    def build_actor(self):\n",
    "        actor = Sequential()\n",
    "        actor.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        actor.add(Dense(self.action_size, activation='softmax',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        actor.summary()\n",
    "        # See note regarding crossentropy in cartpole_reinforce.py\n",
    "        actor.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=Adam(lr=self.actor_lr))\n",
    "        return actor\n",
    "\n",
    "    # critic: state is input and value of state is output of model\n",
    "    def build_critic(self):\n",
    "        critic = Sequential()\n",
    "        critic.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                         kernel_initializer='he_uniform'))\n",
    "        critic.add(Dense(self.value_size, activation='linear',\n",
    "                         kernel_initializer='he_uniform'))\n",
    "        critic.summary()\n",
    "        critic.compile(loss=\"mse\", optimizer=Adam(lr=self.critic_lr))\n",
    "        return critic\n",
    "\n",
    "    # using the output of policy network, pick action stochastically\n",
    "    def get_action(self, state):\n",
    "        policy = self.actor.predict(state, batch_size=1).flatten()\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0]\n",
    "\n",
    "    # update policy network every episode\n",
    "    def train_model(self, state, action, reward, next_state, done):\n",
    "        #TODO update to train the one model\n",
    "        target = np.zeros((1, self.value_size))\n",
    "        advantages = np.zeros((1, self.action_size))\n",
    "\n",
    "        value = self.critic.predict(state)[0]\n",
    "        next_value = self.critic.predict(next_state)[0]\n",
    "\n",
    "        if done:\n",
    "            advantages[0][action] = reward - value\n",
    "            target[0][0] = reward\n",
    "        else:\n",
    "            advantages[0][action] = reward + self.discount_factor * (next_value) - value\n",
    "            target[0][0] = reward + self.discount_factor * next_value\n",
    "\n",
    "        self.actor.fit(state, advantages, epochs=1, verbose=0)\n",
    "        self.critic.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "#TODO change below to work with my code\n",
    "#TODO make it use target network?\n",
    "#TODO make it use experience replay?\n",
    "if __name__ == \"__main__\":\n",
    "    # In case of CartPole-v1, maximum length of episode is 500\n",
    "    env = gym.make('CartPole-v1')\n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    # make A2C agent\n",
    "    agent = A2CAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            # if an action make the episode end, then gives penalty of -100\n",
    "            reward = reward if not done or score == 499 else -100\n",
    "\n",
    "            agent.train_model(state, action, reward, next_state, done)\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # every episode, plot the play time\n",
    "                score = score if score == 500.0 else score + 100\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./save_graph/cartpole_a2c.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score)\n",
    "\n",
    "                # if the mean of scores of last 10 episode is bigger than 490\n",
    "                # stop training\n",
    "                if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "                    sys.exit()\n",
    "\n",
    "        # save the model\n",
    "        if e % 50 == 0:\n",
    "            agent.actor.save_weights(\"./save_model/cartpole_actor.h5\")\n",
    "            agent.critic.save_weights(\"./save_model/cartpole_critic.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
